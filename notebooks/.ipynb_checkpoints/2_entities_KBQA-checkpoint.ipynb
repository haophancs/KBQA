{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection success.\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "dataset_name = 'lcquad'\n",
    "\n",
    "import os\n",
    "os.chdir('/mpqa/KBQA/src')\n",
    "\n",
    "# connect to entity catalog indexed with Lucene \n",
    "from elasticsearch import Elasticsearch\n",
    "from urllib.parse import quote\n",
    "\n",
    "class IndexSearch:\n",
    "    \n",
    "    def __init__(self, index_name):\n",
    "        # set up ES connection\n",
    "        self.es = Elasticsearch()\n",
    "        self.index = index_name\n",
    "        self.type = 'terms'\n",
    "\n",
    "    def match_label(self, string, top=100):\n",
    "        return self.es.search(index=self.index,\n",
    "                              body={\"query\": {\"multi_match\": {\"query\": string,\n",
    "                                                              \"operator\": \"and\",\n",
    "                                                              \"fields\": [\"label^10\", \"label.ngrams\"],\n",
    "                                                              }}},\n",
    "                              size=top, doc_type=self.type)['hits']['hits']\n",
    "\n",
    "    def look_up_by_uri(self, uri, top=1):\n",
    "        results = self.es.search(index=self.index,\n",
    "                              body={\"query\": {\"term\": {\"uri\": quote(uri, safe='():/,')}}},\n",
    "                              size=top, doc_type=self.type)['hits']['hits']\n",
    "        if not results:\n",
    "            # fall back to label match\n",
    "            return self.match_label(uri.split('/')[-1], top=1)\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "e_index = IndexSearch('dbpedia201604e')\n",
    "\n",
    "# set up connection to the MongoDB where the QA dataset is stored\n",
    "# sudo service mongod start (27017 is the default port)\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "class Mongo_Connector():\n",
    "    '''\n",
    "    Wrapper class for some of the pymongo functions: http://api.mongodb.com/python/current/tutorial.html\n",
    "    '''\n",
    "\n",
    "    def __init__(self, db_name, col_name):\n",
    "        # spin up database\n",
    "        self.mongo_client = MongoClient()\n",
    "        self.db = self.mongo_client[db_name]\n",
    "        self.col = self.db[col_name]\n",
    "        print(\"Connection success.\")\n",
    "    \n",
    "    def count_all_docs(self):\n",
    "        count = self.col.count_documents({})\n",
    "        print (\"%d docs\"%count)\n",
    "    \n",
    "    def load_json(self, json_file_path):\n",
    "        with open(json_file_path, \"r\") as json_file:\n",
    "            docs = json.load(json_file)\n",
    "        dataset_size = len(docs)\n",
    "        print (\"%d docs\"%(dataset_size))\n",
    "        self.col.insert_many(docs)\n",
    "\n",
    "    def show_example(self):\n",
    "        pprint.pprint(self.col.find_one())\n",
    "    \n",
    "    def get_sample(self, sample_size=100):\n",
    "        cursor = self.col.find()\n",
    "        if sample_size:\n",
    "            cursor = cursor.limit(sample_size)\n",
    "        return cursor\n",
    "\n",
    "mongo = Mongo_Connector('kbqa', dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing lcquad dataset\n",
      "303131 3271\n",
      "Loaded 4998 lcquad questions\n"
     ]
    }
   ],
   "source": [
    "# load lcquad samples from MongoDB\n",
    "limit = None\n",
    "\n",
    "# prepare data for entity and predicate mention extraction models training via sequence tagging\n",
    "import urllib.parse\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "questions = []\n",
    "question_words = []\n",
    "n_words_distr = []\n",
    "\n",
    "correct_e_spans = []\n",
    "y_e = []\n",
    "correct_entities_uris = []\n",
    "correct_entities_ids = []\n",
    "\n",
    "correct_answers_uris = []\n",
    "correct_answers_ids = []\n",
    "\n",
    "\n",
    "print(\"Preparing %s dataset\"%dataset_name)\n",
    "\n",
    "total_cnt = 0\n",
    "fail_cnt = 0\n",
    "\n",
    "processed = 0\n",
    "\n",
    "while True:\n",
    "    qas = mongo.get_sample(limit).skip(processed)\n",
    "    try:\n",
    "        for q in qas:\n",
    "            # parse question\n",
    "            question_o = q['question']\n",
    "            questions.append(question_o)\n",
    "            words = text_to_word_sequence(question_o)\n",
    "            n_words_distr.append(len(words))\n",
    "            question_words.append(words)\n",
    "\n",
    "            # generate IO tags from mention spans\n",
    "            entity_spans = [e['label'].lower() for e in q['entity mapping']]\n",
    "            correct_e_spans.append(entity_spans)\n",
    "            y_e.append([1 if word in [entity for entity_span in entity_spans for entity in entity_span.split()] else 0 for word in words])\n",
    "\n",
    "            e_uris = [e['uri'].replace(\"'\", \"\") for e in q['entity mapping']]\n",
    "            correct_entities_uris.append(e_uris)\n",
    "            try:\n",
    "                e_ids = [e_index.look_up_by_uri(uri, top=1)[0]['_source']['id'] for uri in e_uris]\n",
    "            except IndexError:\n",
    "                e_ids = []\n",
    "            correct_entities_ids.append(e_ids)\n",
    "            if 'answers' in q:\n",
    "                a_uris = [e_uri.replace(\"'\", \"\") for e_uri in q['answers']]\n",
    "            else:\n",
    "                a_uris = []\n",
    "            correct_answers_uris.append(a_uris)\n",
    "            a_ids = []\n",
    "            for uri in a_uris:\n",
    "                total_cnt += 1\n",
    "                try:\n",
    "                    a_ids.append(e_index.look_up_by_uri(uri, top=1)[0]['_source']['id'])\n",
    "                except:\n",
    "                    # print(\"%s not found in the entity catalog\"%uri)\n",
    "                    fail_cnt += 1\n",
    "            correct_answers_ids.append(a_ids)\n",
    "            processed += 1\n",
    "        break\n",
    "    except CursorNotFound:\n",
    "        print(f\"Lost cursor. Retry with skip {processed} items\")\n",
    "    \n",
    "dataset_size = len(questions)\n",
    "\n",
    "print(total_cnt, fail_cnt)\n",
    "print(\"Loaded %d %s questions\"%(dataset_size, dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which royalty was married to ptolemy XIII Theos Philopator and had mother named Cleopatra V ?\n",
      "['cleopatra v', 'ptolemy xiii theos philopator']\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "['http://dbpedia.org/resource/Cleopatra_V_of_Egypt', 'http://dbpedia.org/resource/Ptolemy_XIII_Theos_Philopator']\n",
      "[8078673, 18811966]\n",
      "['http://dbpedia.org/resource/Cleopatra']\n",
      "[8078652]\n"
     ]
    }
   ],
   "source": [
    "# show sample question\n",
    "i = 5\n",
    "print(questions[i])\n",
    "print(correct_e_spans[i])\n",
    "print(y_e[i])\n",
    "print(correct_entities_uris[i])\n",
    "print(correct_entities_ids[i])\n",
    "print(correct_answers_uris[i])\n",
    "print(correct_answers_ids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct spans\n",
    "\n",
    "Estimate the upper bound for entity scoring function performance on the correct entity spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity match recall estimated on 500 questions @5\n"
     ]
    }
   ],
   "source": [
    "# check if the correct entities are in the subgraph 1-hop away from the top entities\n",
    "# path to KG\n",
    "from hdt import HDTDocument\n",
    "hdt_path = \"/mpqa/indexing/\"\n",
    "hdt_file = 'dbpedia2016-04en.hdt'\n",
    "namespace = \"http://dbpedia.org/\"\n",
    "\n",
    "\n",
    "def evaluate_entity_ranking(_e_spans, indices, top_n):\n",
    "    '''\n",
    "    Estimate ranking accuracy:\n",
    "    n_samples <int> size of the sample questions pool\n",
    "    top_n <int> threshold for the number of top entities \n",
    "    '''\n",
    "    n_correct_entities, n_correct_entities_1hop = 0, 0\n",
    "    n_correct_answers_1hop = 0\n",
    "    # match entities\n",
    "    for i in indices:\n",
    "        top_e_ids = []\n",
    "        \n",
    "        # entities index lookup\n",
    "        for span in _e_spans[i]:\n",
    "            for match in e_index.match_label(span, top=top_n):\n",
    "                top_e_ids.append(match['_source']['id'])\n",
    "        \n",
    "        if set(correct_entities_ids[i]).issubset(set(top_e_ids)):\n",
    "            n_correct_entities += 1\n",
    "        \n",
    "        # extract a subgraph for top entities\n",
    "        kg = HDTDocument(hdt_path+hdt_file)\n",
    "        # all predicates: 1 hop\n",
    "        kg.configure_hops(1, [], namespace, True, True)\n",
    "        entities, _, _ = kg.compute_hops(top_e_ids, 500000, 0)\n",
    "        if set(correct_entities_ids[i]).issubset(set(entities)):\n",
    "            n_correct_entities_1hop += 1\n",
    "        if set(correct_answers_ids[i]).issubset(set(entities)):\n",
    "            n_correct_answers_1hop += 1\n",
    "        kg.remove()\n",
    "\n",
    "\n",
    "    r_entities = float(n_correct_entities) / n_samples\n",
    "    r_entities_1hop = float(n_correct_entities_1hop) / n_samples\n",
    "    r_answers_1hop = float(n_correct_answers_1hop) / n_samples\n",
    "    \n",
    "    return [r_entities, r_entities_1hop, r_answers_1hop]\n",
    "\n",
    "\n",
    "# define sample size for evaluation\n",
    "n_samples = 500\n",
    "top_n_range = [1, 5]\n",
    "print(\"Entity match recall estimated on %d questions @%d\"%(n_samples, top_n_range[1]))\n",
    "# shuffle dataset to get a random sample\n",
    "from random import shuffle\n",
    "index_shuf = list(range(dataset_size))\n",
    "shuffle(index_shuf)\n",
    "index_shuf = index_shuf[:n_samples]\n",
    "assert len(index_shuf) == n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GET http://localhost:9200/dbpedia201604e/terms/_search?size=1 [status:N/A request:10.028s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 384, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 2, in raise_from\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/http/client.py\", line 1379, in getresponse\n",
      "    response.begin()\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/http/client.py\", line 311, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/http/client.py\", line 272, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "socket.timeout: timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py\", line 115, in perform_request\n",
      "    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/util/retry.py\", line 343, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/packages/six.py\", line 686, in reraise\n",
      "    raise value\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 306, in _raise_timeout\n",
      "    raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "ename": "ConnectionTimeout",
     "evalue": "ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Disabled, indicate to re-raise the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out. (read timeout=%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionTimeout\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ab31f759b2ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# recalls at 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mtop_n\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtop_n_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_entity_ranking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_e_spans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_shuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtop_n\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ffcc76c80736>\u001b[0m in \u001b[0;36mevaluate_entity_ranking\u001b[0;34m(_e_spans, indices, top_n)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# entities index lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_e_spans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mtop_e_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-255d0d5c53b6>\u001b[0m in \u001b[0;36mmatch_label\u001b[0;34m(self, string, top)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                               \u001b[0;34m\"fields\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"label^10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label.ngrams\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                               }}},\n\u001b[0;32m---> 25\u001b[0;31m                               size=top, doc_type=self.type)['hits']['hits']\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlook_up_by_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/elasticsearch/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, index, doc_type, body, params)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_all'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         return self.transport.perform_request('GET', _make_path(index,\n\u001b[0;32m--> 632\u001b[0;31m             doc_type, '_search'), params=params, body=body)\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     @query_params('_source', '_source_exclude', '_source_include',\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTransportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mpqa/miniconda3/envs/kbqa/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N/A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TIMEOUT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N/A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionTimeout\u001b[0m: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10))"
     ]
    }
   ],
   "source": [
    "# evaluate on correct entity spans\n",
    "top_n = top_n_range[0]\n",
    "results = [[0, 0, 0]]  # recalls at 0\n",
    "while top_n <= top_n_range[1]:\n",
    "    results.append(evaluate_entity_ranking(correct_e_spans, index_shuf, top_n))\n",
    "    top_n += 1\n",
    "    \n",
    "# show result\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "print(results)\n",
    "\n",
    "# plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(results[0], label='Entity match')\n",
    "plt.plot(results[1], label='1-hop entity match')\n",
    "plt.plot(results[2], label='1-hop answer match')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test configure_hops arg_4 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the correct entities are in the subgraph 1-hop away from the top entities\n",
    "# path to KG\n",
    "from hdt import HDTDocument\n",
    "hdt_path = \"/mpqa/indexing/\"\n",
    "hdt_file = 'dbpedia2016-04en.hdt'\n",
    "namespace = \"http://dbpedia.org/\"\n",
    "\n",
    "\n",
    "def evaluate_entity_ranking(_e_spans, indices, top_n):\n",
    "    '''\n",
    "    Estimate ranking accuracy:\n",
    "    n_samples <int> size of the sample questions pool\n",
    "    top_n <int> threshold for the number of top entities \n",
    "    '''\n",
    "    n_correct_entities, n_correct_entities_1hop = 0, 0\n",
    "    n_correct_answers_1hop = 0\n",
    "    # match entities\n",
    "    for i in indices:\n",
    "        top_e_ids = []\n",
    "        \n",
    "        # entities index lookup\n",
    "        for span in _e_spans[i]:\n",
    "            for match in e_index.match_label(span, top=top_n):\n",
    "                top_e_ids.append(match['_source']['id'])\n",
    "        \n",
    "        if set(correct_entities_ids[i]).issubset(set(top_e_ids)):\n",
    "            n_correct_entities += 1\n",
    "        \n",
    "        # extract a subgraph for top entities\n",
    "        kg = HDTDocument(hdt_path+hdt_file)\n",
    "        # all predicates: 1 hop\n",
    "        try:\n",
    "            kg.configure_hops(1, [], namespace, True, False)\n",
    "            entities, _, _ = kg.compute_hops(top_e_ids, 500000, 0)\n",
    "            if set(correct_entities_ids[i]).issubset(set(entities)):\n",
    "                n_correct_entities_1hop += 1\n",
    "            if set(correct_answers_ids[i]).issubset(set(entities)):\n",
    "                n_correct_answers_1hop += 1\n",
    "            kg.remove()\n",
    "        except:\n",
    "            print('Timeout')\n",
    "\n",
    "\n",
    "    r_entities = float(n_correct_entities) / n_samples\n",
    "    r_entities_1hop = float(n_correct_entities_1hop) / n_samples\n",
    "    r_answers_1hop = float(n_correct_answers_1hop) / n_samples\n",
    "    \n",
    "    return [r_entities, r_entities_1hop, r_answers_1hop]\n",
    "\n",
    "\n",
    "# define sample size for evaluation\n",
    "n_samples = 500\n",
    "top_n_range = [1, 5]\n",
    "print(\"Entity match recall estimated on %d questions @%d\"%(n_samples, top_n_range[1]))\n",
    "# shuffle dataset to get a random sample\n",
    "from random import shuffle\n",
    "index_shuf = list(range(dataset_size))\n",
    "shuffle(index_shuf)\n",
    "index_shuf = index_shuf[:n_samples]\n",
    "assert len(index_shuf) == n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on correct entity spans\n",
    "top_n = top_n_range[0]\n",
    "results = [[0, 0, 0]]  # recalls at 0\n",
    "while top_n <= top_n_range[1]:\n",
    "    results.append(evaluate_entity_ranking(correct_e_spans, index_shuf, top_n))\n",
    "    top_n += 1\n",
    "    \n",
    "# show result\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "print(results)\n",
    "\n",
    "# plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(results[0], label='Entity match')\n",
    "plt.plot(results[1], label='1-hop entity match')\n",
    "plt.plot(results[2], label='1-hop answer match')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted spans\n",
    "\n",
    "Estimate performance on the extracted entity spans using the mention extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained entity mention extraction model\n",
    "embeddings_choice = 'glove840B300d'\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(model_settings):\n",
    "    # architecture\n",
    "    input = Input(shape=(model_settings['max_len'],))\n",
    "    model = Embedding(input_dim=model_settings['n_words']+1, output_dim=model_settings['emb_dim'],\n",
    "                      weights=[model_settings['embeddings']],\n",
    "                      input_length=model_settings['max_len'], mask_zero=True, trainable=False)(input)\n",
    "    model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "    model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "    crf = CRF(model_settings['n_tags'])  # CRF layer\n",
    "    out = crf(model)  # output\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# load model settings\n",
    "import pickle as pkl\n",
    "with open('%s_%s.pkl'%(dataset_name, embeddings_choice), 'rb') as f:\n",
    "    model_settings = pkl.load(f)\n",
    "model = build_model(model_settings)\n",
    "\n",
    "# load weights\n",
    "model_name = 'entity_model'\n",
    "model.load_weights('../models/'+model_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate entity span detection\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def evaluate_entity_span_extraction(show_errors=False):\n",
    "    n_correct = 0\n",
    "    questions_e_spans = []\n",
    "    for i, words in enumerate(question_words):\n",
    "        x_test_sent = pad_sequences(sequences=[[model_settings['word2idx'].get(w, 'unk') for w in words]],\n",
    "                                    padding=\"post\", value=0, maxlen=model_settings['max_len'])\n",
    "        p = model.predict(np.array([x_test_sent[0]]))\n",
    "        p = np.argmax(p, axis=-1)[0]\n",
    "\n",
    "        e_span, e_spans = [], []\n",
    "        for w, pred in zip(words, p):\n",
    "            if pred > 0:\n",
    "                e_span.append(w)\n",
    "            elif e_span:\n",
    "                e_spans.append(\" \".join(e_span))\n",
    "                e_span = []\n",
    "        # add last span\n",
    "        if e_span:\n",
    "            e_spans.append(\" \".join(e_span))\n",
    "            e_span = []\n",
    "\n",
    "        if set(correct_e_spans[i]) == set(e_spans):\n",
    "            n_correct += 1\n",
    "        elif show_errors:\n",
    "            print('\\n')\n",
    "            print(set(e_spans))\n",
    "            # show correct spans\n",
    "            print(set(correct_e_spans[i]))\n",
    "        questions_e_spans.append(e_spans)\n",
    "    p = float(n_correct) / dataset_size\n",
    "    print(\"\\nAcc: %.2f \"%(p))\n",
    "    return questions_e_spans\n",
    "\n",
    "# evaluate\n",
    "print(\"Accuracy estimated on %d questions\"%(dataset_size))\n",
    "extracted_e_spans = evaluate_entity_span_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on correct entity spans\n",
    "top_n = top_n_range[0]\n",
    "results = [[0, 0, 0]]  # recalls at 0\n",
    "while top_n <= top_n_range[1]:\n",
    "    results.append(evaluate_entity_ranking(extracted_e_spans, index_shuf, top_n))\n",
    "    top_n += 1\n",
    "    \n",
    "# show result\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "print(results)\n",
    "\n",
    "# plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(results[0], label='Entity match')\n",
    "plt.plot(results[1], label='1-hop entity match')\n",
    "plt.plot(results[2], label='1-hop answer match')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
