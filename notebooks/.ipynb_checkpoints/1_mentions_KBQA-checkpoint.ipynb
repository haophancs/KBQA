{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentions Extraction for KBQA\n",
    "\n",
    "Parsing question into entities and predicate mentions text spans.\n",
    "\n",
    "bi-LSTM + CRF  \n",
    "https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf  \n",
    "https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/  \n",
    "https://github.com/SNUDerek/multiLSTM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "dataset_name = 'lcquad'\n",
    "embeddings_choice = 'glove840B300d'\n",
    "\n",
    "import numpy as np\n",
    "# set seed\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import os\n",
    "os.chdir('/mpqa/KBQA/src')\n",
    "\n",
    "# load word frequencies\n",
    "import pickle\n",
    "#wfd = pickle.load(open(\"wfd.pkl\", \"rb\"))\n",
    "\n",
    "# make sure Keras is using the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# load pre-trained word embeddings\n",
    "from pymagnitude import *\n",
    "embeddings_path = \"/mpqa/KBQA/data/embeddings/\"\n",
    "embeddings = {'glove840B300d': \"glove.840B.300d.magnitude\"}\n",
    "vectors = Magnitude(embeddings_path + embeddings[embeddings_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lcquad span annotations\n",
    "# wget https://raw.githubusercontent.com/AskNowQA/EARL/master/data/lcquad.json\n",
    "import json\n",
    "with open(\"../data/%s.json\"%dataset_name, \"r\", encoding='utf-8') as file:\n",
    "    qas = json.load(file)\n",
    "\n",
    "# prepare data for entity and predicate mention extraction models training via sequence tagging\n",
    "limit = 5000\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "questions = []\n",
    "n_words_distr = []\n",
    "\n",
    "e_spans = []\n",
    "y_e = []\n",
    "correct_entities_uris = []\n",
    "\n",
    "p_spans = []\n",
    "y_p = []\n",
    "correct_predicates_uris = []\n",
    "\n",
    "for q in qas[:limit]:\n",
    "    # parse question\n",
    "    question_o = q['question']\n",
    "    words = text_to_word_sequence(question_o)\n",
    "    n_words_distr.append(len(words))\n",
    "    questions.append(words)\n",
    "\n",
    "    # generate IO tags from mention spans\n",
    "    entity_spans = [e['label'].lower().split() for e in q['entity mapping']]\n",
    "    e_spans.append(entity_spans)\n",
    "    y_e.append([1 if word in [entity for entity_span in entity_spans for entity in entity_span] else 0 for word in words])\n",
    "    correct_entities_uris.append([e['uri'] for e in q['entity mapping']])\n",
    "    \n",
    "    predicate_spans = [e['label'].lower().split() for e in q['predicate mapping']]\n",
    "    p_spans.append(predicate_spans)\n",
    "    y_p.append([1 if word in [entity for entity_span in predicate_spans for entity in entity_span] else 0 for word in words])\n",
    "    correct_predicates_uris.append([e['uri'] for e in q['predicate mapping']])\n",
    "\n",
    "dataset_size = len(questions)\n",
    "print(\"Loaded %d/%d %s questions\"%(dataset_size, len(qas), dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample question\n",
    "i = 1\n",
    "question_o = qas[i]['question']\n",
    "print(question_o)\n",
    "print('\\n')\n",
    "print(e_spans[i])\n",
    "print(y_e[i])\n",
    "print(correct_entities_uris[i])\n",
    "print('\\n')\n",
    "print(p_spans[i])\n",
    "print(y_p[i])\n",
    "print(correct_predicates_uris[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained embeddings for question words\n",
    "words = list(set([word for q in questions for word in q]))\n",
    "n_words = len(words)\n",
    "print(\"Number of unique words %d\"%len(words))\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "\n",
    "# dataset parameters for training the model\n",
    "max_len = max(n_words_distr)\n",
    "print(\"Maximum question length in the dataset: %d\"%max_len)\n",
    "n_tags = 2\n",
    "\n",
    "# prepare data and pad the max length with 0s\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2idx[w] for w in s] for s in questions]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
    "\n",
    "# load embeddings into matrix\n",
    "import math\n",
    "word_embedding_matrix = np.zeros((n_words+1, vectors.dim))\n",
    "\n",
    "n_oov = 0\n",
    "\n",
    "for w in word2idx:\n",
    "    # get the word vector from the embedding model\n",
    "    if w in vectors:\n",
    "        word_vector = vectors.query(w)\n",
    "    # OOV word\n",
    "    else:\n",
    "        n_oov += 1\n",
    "        word_vector = vectors.query('unk')\n",
    "    word_embedding_matrix[word2idx[w]] = word_vector\n",
    "\n",
    "# loaded vector # may be lower than total vocab due to w2v settings\n",
    "print('%d OOV words'%n_oov)\n",
    "\n",
    "model_settings = {'embeddings': word_embedding_matrix, 'word2idx': word2idx,\n",
    "                  'max_len': max_len, 'n_words': n_words, 'n_tags': n_tags, 'emb_dim': vectors.dim}\n",
    "\n",
    "# save model settings\n",
    "import pickle as pkl\n",
    "f = open('%s_%s.pkl'%(dataset_name, embeddings_choice), 'wb')\n",
    "pkl.dump(model_settings, f, -1)\n",
    "f.close()\n",
    "print(\"Model settings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build biLSTM-CRF model for mention extraction\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(model_settings):\n",
    "    # architecture\n",
    "    input = Input(shape=(model_settings['max_len'],))\n",
    "    model = Embedding(input_dim=model_settings['n_words']+1, output_dim=model_settings['emb_dim'],\n",
    "                      weights=[model_settings['embeddings']],\n",
    "                      input_length=model_settings['max_len'], mask_zero=True, trainable=False)(input)\n",
    "    model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "    model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "    crf = CRF(model_settings['n_tags'])  # CRF layer\n",
    "    out = crf(model)  # output\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model(model_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train entity mention span model\n",
    "y = y_e\n",
    "modelname = 'entity_model'\n",
    "\n",
    "# prepare data\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=0)\n",
    "from keras.utils import to_categorical\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "# split dataset into training and testing subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(\"Training on %d samples testing on %d samples\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
    "cb_redlr = ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "cb_early = EarlyStopping(monitor='val_crf_viterbi_accuracy', min_delta=0, patience=5, verbose=1)\n",
    "cb_chkpt = ModelCheckpoint('../checkpoints/_'+modelname+'{epoch:02d}-{val_crf_viterbi_accuracy:.2f}.h5', verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "\n",
    "callbacks_list=[cb_redlr, cb_early, cb_chkpt]\n",
    "\n",
    "# start training\n",
    "log = model.fit(X_train, np.array(y_train), batch_size=32, epochs=50,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies\n",
    "import pandas as pd\n",
    "hist = pd.DataFrame(log.history)\n",
    "# print(hist)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(hist[\"crf_viterbi_accuracy\"], label='Training accuracy')\n",
    "plt.plot(hist[\"val_crf_viterbi_accuracy\"], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "from keras_contrib.utils import save_load_utils\n",
    "with open('../models/' + modelname + '.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "# save weights\n",
    "save_load_utils.save_all_weights(model, '../models/'+modelname+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model settings\n",
    "import pickle as pkl\n",
    "with open('%s_%s.pkl'%(dataset_name, embeddings_choice), 'rb') as f:\n",
    "    model_settings = pkl.load(f)\n",
    "model = build_model(model_settings)\n",
    "\n",
    "# load weights\n",
    "model.load_weights('../models/'+modelname+'.h5')\n",
    "\n",
    "# evaluate model on the test set\n",
    "test_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "idx2tag = {1: 'I', 0: 'O'}\n",
    "pred_labels = [[idx2tag[np.argmax(p)] for p in pred_i] for pred_i in test_pred]\n",
    "test_labels = [[idx2tag[np.argmax(p)] for p in pred_i] for pred_i in y_test]\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels, average='weighted')))\n",
    "print(\"Precision-score: {:.1%}\".format(precision_score(test_labels, pred_labels, average='weighted')))\n",
    "print(\"Recall-score: {:.1%}\".format(recall_score(test_labels, pred_labels, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample test prediction\n",
    "# i = 1\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_test[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-1], t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train entity mention span model\n",
    "y = y_p\n",
    "modelname = 'predicate_model'\n",
    "\n",
    "# prepare data\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=0)\n",
    "from keras.utils import to_categorical\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "# split dataset into training and testing subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(\"Training on %d samples testing on %d samples\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
    "cb_redlr = ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "cb_early = EarlyStopping(monitor='val_crf_viterbi_accuracy', min_delta=0, patience=5, verbose=1)\n",
    "cb_chkpt = ModelCheckpoint('../checkpoints/_'+modelname+'{epoch:02d}-{val_crf_viterbi_accuracy:.2f}.h5', verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "\n",
    "callbacks_list=[cb_redlr, cb_early, cb_chkpt]\n",
    "\n",
    "# start training\n",
    "log = model.fit(X_train, np.array(y_train), batch_size=32, epochs=50,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies\n",
    "import pandas as pd\n",
    "hist = pd.DataFrame(log.history)\n",
    "# print(hist)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(hist[\"crf_viterbi_accuracy\"], label='Training accuracy')\n",
    "plt.plot(hist[\"val_crf_viterbi_accuracy\"], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "from keras_contrib.utils import save_load_utils\n",
    "with open('../models/' + modelname + '.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "# save weights\n",
    "save_load_utils.save_all_weights(model, '../models/'+modelname+'.h5')\n",
    "\n",
    "# load weights\n",
    "model.load_weights('../models/'+modelname+'.h5')\n",
    "\n",
    "# evaluate model on the test set\n",
    "test_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "idx2tag = {1: 'I', 0: 'O'}\n",
    "pred_labels = [[idx2tag[np.argmax(p)] for p in pred_i] for pred_i in test_pred]\n",
    "test_labels = [[idx2tag[np.argmax(p)] for p in pred_i] for pred_i in y_test]\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample test prediction\n",
    "i = 1\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_test[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-1], t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
