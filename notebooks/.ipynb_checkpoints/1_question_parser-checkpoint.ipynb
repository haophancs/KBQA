{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question parser\n",
    "\n",
    "Parse question into tokens assigned to 4 groups: \n",
    "* entities and predicates for the 1st hop (E1 P1)  \n",
    "* entities and predicates for the 2nd hop (E2 P2)  \n",
    "\n",
    "Then train a supervised sequence tagging model to extract the mention text spans.\n",
    "\n",
    "Reference: https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "dataset_name = 'lcquad'\n",
    "embeddings_choice = 'glove840B300d'\n",
    "\n",
    "import os\n",
    "os.chdir('/mpqa/KBQA/src')  # path to working directory for saving models\n",
    "\n",
    "# connect to MongoDB (27017 is the default port) to access the dataset\n",
    "# sudo service mongod start \n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class Mongo_Connector():\n",
    "    '''\n",
    "    Wrapper class for some of the pymongo functions: http://api.mongodb.com/python/current/tutorial.html\n",
    "    '''\n",
    "\n",
    "    def __init__(self, db_name, col_name):\n",
    "        # spin up database\n",
    "        self.mongo_client = MongoClient()\n",
    "        self.col = self.mongo_client[db_name][col_name]\n",
    "        \n",
    "    def get_sample(self, limit=100):\n",
    "        '''\n",
    "        Set limit to None to get all docs\n",
    "        '''\n",
    "        cursor = self.col.find()\n",
    "        if limit:\n",
    "            cursor = cursor.limit(limit)\n",
    "        return cursor\n",
    "\n",
    "\n",
    "mongo = Mongo_Connector('kbqa', dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Encode questions with IO tokens: E1E2P1P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = None\n",
    "verbose = False\n",
    "\n",
    "tags = ['O', 'E', 'P']  # + p_tags\n",
    "modelname = 'joint'  # joint predicate\n",
    "\n",
    "import re\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def parse_question(question_o, spans):\n",
    "    # exclude duplicate spans\n",
    "    spans = list(set(spans))\n",
    "    # sort the mention spans into the correct sequence order in the question\n",
    "    spans.sort(key=lambda tup: tup[0])\n",
    "    # parse question into mention spans\n",
    "    cursor_idx = 0\n",
    "    words, ye = [], []\n",
    "    for span in spans:\n",
    "        idx_start, idx_end, tag = span\n",
    "        # spacing\n",
    "        space = text_to_word_sequence(question_o[cursor_idx:idx_start])\n",
    "        words.extend(space)\n",
    "        ye.extend([''] * len(space))\n",
    "        # mention\n",
    "        mention = text_to_word_sequence(question_o[idx_start:idx_end])\n",
    "        words.extend(mention)\n",
    "        ye.extend([tag] * len(mention))\n",
    "        # move cursor to the end of the current annotation span\n",
    "        cursor_idx = idx_end\n",
    "    # add the remaining spacing\n",
    "    space = text_to_word_sequence(question_o[cursor_idx:])\n",
    "    words.extend(space)\n",
    "    ye.extend([''] * len(space))\n",
    "    if verbose:\n",
    "        print(spans)\n",
    "        print(words)\n",
    "        print(ye)\n",
    "    assert len(words) == len(ye)\n",
    "    return words, ye\n",
    "\n",
    "\n",
    "samples = mongo.get_sample(limit=limit)\n",
    "# iterate over the cursor\n",
    "questions = []\n",
    "n_words_distr = []\n",
    "labels = []\n",
    "count = 0\n",
    "question_types = []  # indicate complex questions by number of hops\n",
    "for doc in samples:\n",
    "    _complex = 0  # default indicator for simple questions\n",
    "    # get sample annotations\n",
    "    question_o = doc['question']\n",
    "    # get URI distribution across the hops\n",
    "    e1, p1 = doc['1hop']\n",
    "    e2, p2 = doc['2hop']\n",
    "    \n",
    "    # collect span annotations\n",
    "    e_spans = []\n",
    "    p_spans = []\n",
    "    # fix: some entities are erroneously annotated as predicates\n",
    "    for e in doc['entity mapping']+doc['predicate mapping']:\n",
    "        # fix: use labels to find substrings since some of the span indices annotations are incorrect\n",
    "        matches = re.finditer(re.escape(e['label'].lower()), question_o.lower())\n",
    "        # pick tag for this mention\n",
    "        if e['uri'] in e1:\n",
    "            e_spans.extend([(match.start(), match.end(), 'E1') for match in matches])\n",
    "        elif e['uri'] in e2:\n",
    "            e_spans.extend([(match.start(), match.end(), 'E2') for match in matches])\n",
    "            _complex = 1  # indicate complex question\n",
    "        elif e['uri'] in p1:\n",
    "            p_spans.extend([(match.start(), match.end(), 'P1') for match in matches])\n",
    "        elif e['uri'] in p2:\n",
    "            p_spans.extend([(match.start(), match.end(), 'P2') for match in matches])\n",
    "            _complex = 1  # indicate complex question\n",
    "\n",
    "    if modelname == 'entity':\n",
    "        words, _y = parse_question(question_o, e_spans)\n",
    "    elif modelname == 'predicate':\n",
    "        words, _y = parse_question(question_o, p_spans)\n",
    "    elif modelname == 'joint':\n",
    "        words_e, y_e = parse_question(question_o, e_spans)\n",
    "        words_p, y_p = parse_question(question_o, p_spans)\n",
    "        if words_e == words_p:\n",
    "            words = words_e\n",
    "            _y = []\n",
    "            for tag_e, tag_p in zip(y_e, y_p):\n",
    "                if tag_e == '' or tag_p == '':\n",
    "                    _y.append(tag_e+tag_p)\n",
    "                else:\n",
    "                    _y.append(tag_e) # use the entity tag when tags overlap\n",
    "                \n",
    "    #     add the sample to the dataset \n",
    "    questions.append(words)\n",
    "    n_words_distr.append(len(words))\n",
    "    labels.append(_y)\n",
    "    question_types.append(_complex)\n",
    "    count += 1\n",
    "\n",
    "dataset_size = len(questions)\n",
    "print(\"Loaded %d %s questions\"%(dataset_size, dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample question\n",
    "i = 200\n",
    "print(questions[i])\n",
    "print(labels[i])\n",
    "print(question_types[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings for question semantic representation\n",
    "from pymagnitude import *\n",
    "embeddings_path = \"/mpqa/KBQA/data/embeddings/\"\n",
    "embeddings = {'glove840B300d': \"glove.840B.300d.magnitude\"}\n",
    "vectors = Magnitude(embeddings_path + embeddings[embeddings_choice])\n",
    "\n",
    "words = list(set([word for q in questions for word in q]))\n",
    "n_words = len(words)\n",
    "print(\"Number of unique words %d\"%len(words))\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "\n",
    "# dataset parameters for training the model\n",
    "max_len = max(n_words_distr)\n",
    "print(\"Maximum question length in the dataset: %d\"%max_len)\n",
    "\n",
    "# prepare data and pad the max length with 0s\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2idx[w] for w in s] for s in questions]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
    "\n",
    "# encode tags\n",
    "n_tags = len(tags)\n",
    "y = [[tags.index(tag[0]) if tag else 0 for tag in s] for s in labels]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=0)\n",
    "from keras.utils import to_categorical\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "# load embeddings into matrix\n",
    "import math\n",
    "word_embedding_matrix = np.zeros((n_words+1, vectors.dim))\n",
    "\n",
    "n_oov = 0\n",
    "\n",
    "for w in word2idx:\n",
    "    # get the word vector from the embedding model\n",
    "    if w in vectors:\n",
    "        word_vector = vectors.query(w)\n",
    "    # OOV word\n",
    "    else:\n",
    "        n_oov += 1\n",
    "        word_vector = vectors.query('unk')\n",
    "    word_embedding_matrix[word2idx[w]] = word_vector\n",
    "\n",
    "# loaded vector # may be lower than total vocab due to w2v settings\n",
    "print('%d OOV words'%n_oov)\n",
    "\n",
    "model_settings = {'embeddings': word_embedding_matrix, 'word2idx': word2idx,\n",
    "                  'max_len': max_len, 'n_words': n_words, 'n_tags': n_tags, 'emb_dim': vectors.dim}\n",
    "\n",
    "# save model settings\n",
    "import pickle as pkl\n",
    "f = open('../models/%s_%s.pkl'%(dataset_name, embeddings_choice), 'wb')\n",
    "pkl.dump(model_settings, f, -1)\n",
    "f.close()\n",
    "print(\"Model settings saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mention extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and testing subsets\n",
    "test_size = 0.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# fix random seed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                    stratify=question_types, random_state=103232)\n",
    "print(\"Training on %d samples testing on %d samples\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-LSTM CRF model architecture\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(model_settings):\n",
    "    # architecture\n",
    "    input = Input(shape=(model_settings['max_len'],))\n",
    "    model = Embedding(input_dim=model_settings['n_words']+1, output_dim=model_settings['emb_dim'],\n",
    "                      weights=[model_settings['embeddings']],\n",
    "                      input_length=model_settings['max_len'], mask_zero=True, trainable=False)(input)\n",
    "    model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "    model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "    crf = CRF(model_settings['n_tags'])  # CRF layer\n",
    "    out = crf(model)  # output\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# load model settings\n",
    "import pickle as pkl\n",
    "with open('../models/%s_%s.pkl'%(dataset_name, embeddings_choice), 'rb') as f:\n",
    "    model_settings = pkl.load(f)\n",
    "model = build_model(model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
    "cb_redlr = ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "cb_early = EarlyStopping(monitor='val_crf_viterbi_accuracy', min_delta=0, patience=5, verbose=1)\n",
    "cb_chkpt = ModelCheckpoint('../checkpoints/_'+modelname+'{epoch:02d}-{val_crf_viterbi_accuracy:.2f}.h5', verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "\n",
    "callbacks_list=[cb_redlr, cb_early, cb_chkpt]\n",
    "\n",
    "# start training\n",
    "log = model.fit(X_train, np.array(y_train), batch_size=32, epochs=50,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies\n",
    "import pandas as pd\n",
    "hist = pd.DataFrame(log.history)\n",
    "# print(hist)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(hist[\"crf_viterbi_accuracy\"], label='Training accuracy')\n",
    "plt.plot(hist[\"val_crf_viterbi_accuracy\"], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on the test set\n",
    "test_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "pred_labels = [[tags[np.argmax(p)] for p in pred_i] for pred_i in test_pred]\n",
    "test_labels = [[tags[np.argmax(p)] for p in pred_i] for pred_i in y_test]\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels, average='weighted')))\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample test prediction\n",
    "i = 15\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_test[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-1], t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "from keras_contrib.utils import save_load_utils\n",
    "with open('../models/' + modelname + '.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "# save weights\n",
    "save_load_utils.save_all_weights(model, '../models/'+modelname+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'complex_questions'\n",
    "\n",
    "# split dataset into training and testing subsets\n",
    "test_size = 0.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "# fix random seed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, question_types, test_size=test_size,\n",
    "                                                    stratify=question_types, random_state=103232)\n",
    "print(\"Training on %d samples testing on %d samples\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict question type\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(model_settings):\n",
    "    # architecture\n",
    "    input = Input(shape=(model_settings['max_len'],))\n",
    "    model = Embedding(input_dim=model_settings['n_words']+1, output_dim=model_settings['emb_dim'],\n",
    "                      weights=[model_settings['embeddings']],\n",
    "                      input_length=model_settings['max_len'], mask_zero=True, trainable=False)(input)\n",
    "    model = Bidirectional(LSTM(units=50, return_sequences=False,\n",
    "                               recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "    out = Dense(1, kernel_initializer='normal', activation='sigmoid')(model)  # output\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# load model settings\n",
    "import pickle as pkl\n",
    "with open('%s_%s.pkl'%(dataset_name, embeddings_choice), 'rb') as f:\n",
    "    model_settings = pkl.load(f)\n",
    "model = build_model(model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
    "cb_redlr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "cb_early = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1)\n",
    "cb_chkpt = ModelCheckpoint('../checkpoints/_'+modelname+'{epoch:02d}-{val_acc:.2f}.h5',\n",
    "                           verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "\n",
    "callbacks_list=[cb_redlr, cb_early, cb_chkpt]\n",
    "\n",
    "# start training: weight the classes proportional\n",
    "log = model.fit(X_train, np.array(y_train), batch_size=32, epochs=50,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_split=0.1, verbose=1, class_weight={0: 1., 1: 9.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "# np.set_printoptions(precision=4, suppress=True)\n",
    "# eval_results = model.evaluate(X_test, y_test, verbose=0) \n",
    "# print(\"\\nLoss, accuracy on test data: \")\n",
    "# print(\"%0.4f %0.2f%%\" % (eval_results[0], \\\n",
    "#   eval_results[1]*100))\n",
    "\n",
    "# evaluate model on the test set\n",
    "test_pred = model.predict(X_test, verbose=1)\n",
    "y_pred = [1 if pred > 0.5 else 0 for pred in test_pred]\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample test prediction\n",
    "i = 30\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "q = \" \".join([words[w-1] for w in X_test[i] if w != 0])\n",
    "print(q)\n",
    "print(\"%.2f\"%p[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "from keras_contrib.utils import save_load_utils\n",
    "with open('../models/' + modelname + '.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "# save weights\n",
    "save_load_utils.save_all_weights(model, '../models/'+modelname+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
